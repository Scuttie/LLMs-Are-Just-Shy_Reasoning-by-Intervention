{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Login"
      ],
      "metadata": {
        "id": "5CvHHtAR3I48"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3Ujyrlv3DQ5"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# G-NLL Analysis"
      ],
      "metadata": {
        "id": "NqoQu1yA3LEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "def load_llama7b_model(model_path, device='cuda'):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    model.eval()\n",
        "    return tokenizer, model\n",
        "\n",
        "def generate_text_gnll_once(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    max_length=50,\n",
        "    top_p=0.9,\n",
        "    temperature=1.0,\n",
        "    device='cuda'\n",
        "):\n",
        "    \"\"\"\n",
        "    1회 샘플링 디코딩하여,\n",
        "    생성된 시퀀스와 G-NLL을 반환하는 함수.\n",
        "\n",
        "    Sampling(Top-p, Temperature).\n",
        "    \"\"\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    generated_ids = input_ids.clone()\n",
        "    total_gnll = 0.0\n",
        "\n",
        "    for step in range(max_length):\n",
        "        outputs = model(input_ids=generated_ids)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        last_token_logits = logits[:, -1, :]\n",
        "        scaled_logits = last_token_logits / temperature\n",
        "\n",
        "        sorted_logits, sorted_indices = torch.sort(scaled_logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "        idx_remove = (cumulative_probs > top_p)\n",
        "        if idx_remove.any():\n",
        "            first_true_idx = torch.nonzero(idx_remove, as_tuple=True)[1][0].item()\n",
        "            sorted_logits[0, first_true_idx+1:] = float('-inf')\n",
        "\n",
        "        re_sorted_logits = torch.full_like(scaled_logits, float('-inf'))\n",
        "        re_sorted_logits[0, sorted_indices] = sorted_logits[0]\n",
        "\n",
        "        probs = torch.softmax(re_sorted_logits, dim=-1)\n",
        "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        next_token_prob = probs[0, next_token_id]\n",
        "        total_gnll += -torch.log(next_token_prob).item()\n",
        "\n",
        "        generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
        "\n",
        "        if tokenizer.eos_token_id is not None:\n",
        "            if next_token_id.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    generated_text = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)\n",
        "    return generated_text, total_gnll\n",
        "\n",
        "def sample_n_times(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    n=5,\n",
        "    max_length=50,\n",
        "    top_p=0.9,\n",
        "    temperature=1.0,\n",
        "    device='cuda'\n",
        "):\n",
        "    \"\"\"\n",
        "    같은 prompt에 대해 N회 샘플링 디코딩을 수행하고,\n",
        "    각 결과와 G-NLL을 리스트로 반환.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for i in range(n):\n",
        "        gen_text, gnll_value = generate_text_gnll_once(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            prompt=prompt,\n",
        "            max_length=max_length,\n",
        "            top_p=top_p,\n",
        "            temperature=temperature,\n",
        "            device=device\n",
        "        )\n",
        "        results.append((gen_text, gnll_value))\n",
        "    return results\n",
        "\n",
        "def analyze_gnll_distribution(results):\n",
        "    \"\"\"\n",
        "    여러 번 샘플링한 결과들에 대한 G-NLL 분포를 분석.\n",
        "    결과:\n",
        "      - mean, std, min, max,\n",
        "      - 가장 G-NLL 낮은/높은 시퀀스\n",
        "    \"\"\"\n",
        "    gnll_values = [r[1] for r in results]\n",
        "    mean_gnll = float(np.mean(gnll_values))\n",
        "    std_gnll = float(np.std(gnll_values))\n",
        "    min_gnll = float(np.min(gnll_values))\n",
        "    max_gnll = float(np.max(gnll_values))\n",
        "\n",
        "    idx_min = int(np.argmin(gnll_values))\n",
        "    idx_max = int(np.argmax(gnll_values))\n",
        "    best_text = results[idx_min][0]\n",
        "    worst_text = results[idx_max][0]\n",
        "\n",
        "    return {\n",
        "        \"mean_gnll\": mean_gnll,\n",
        "        \"std_gnll\": std_gnll,\n",
        "        \"min_gnll\": min_gnll,\n",
        "        \"max_gnll\": max_gnll,\n",
        "        \"best_text\": best_text,\n",
        "        \"worst_text\": worst_text\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    tokenizer, model = load_llama7b_model(model_path, device=device)\n",
        "\n",
        "    prompt = \"What is 3+3*2/(1+5)?\"\n",
        "\n",
        "    n_samples = 5\n",
        "    results = sample_n_times(\n",
        "        model, tokenizer,\n",
        "        prompt=prompt,\n",
        "        n=n_samples,\n",
        "        max_length=500,\n",
        "        top_p=0.9,\n",
        "        temperature=1.0,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    analysis = analyze_gnll_distribution(results)\n",
        "\n",
        "    print(f\"=== 샘플링 {n_samples}회 결과 ===\")\n",
        "    for idx, (txt, gnll_val) in enumerate(results):\n",
        "        print(f\"[Sample {idx+1}] G-NLL={gnll_val:.4f}\")\n",
        "        print(f\"         {txt}\\n\")\n",
        "\n",
        "    print(\"=== G-NLL 통계량 ===\")\n",
        "    print(f\"Mean   : {analysis['mean_gnll']:.4f}\")\n",
        "    print(f\"Std    : {analysis['std_gnll']:.4f}\")\n",
        "    print(f\"Min    : {analysis['min_gnll']:.4f}\")\n",
        "    print(f\"Max    : {analysis['max_gnll']:.4f}\")\n",
        "\n",
        "    print(\"\\n--- G-NLL 가장 낮은 시퀀스 (모델이 비교적 확신을 보인 샘플) ---\")\n",
        "    print(analysis[\"best_text\"])\n",
        "\n",
        "    print(\"\\n--- G-NLL 가장 높은 시퀀스 (모델이 상대적으로 불확실했던 샘플) ---\")\n",
        "    print(analysis[\"worst_text\"])\n"
      ],
      "metadata": {
        "id": "NuV2M2Ml3SCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalize G-NLL"
      ],
      "metadata": {
        "id": "8INxezyH3eSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "def check_sentence_end(decoded_text_so_far: str):\n",
        "    decoded_text_so_far = decoded_text_so_far.strip()\n",
        "    if len(decoded_text_so_far) == 0:\n",
        "        return False\n",
        "    last_char = decoded_text_so_far[-1]\n",
        "    return (last_char in ['.', '?', '!'])\n",
        "\n",
        "def sample_single_sentence(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    context_ids,\n",
        "    top_p=0.9,\n",
        "    temperature=1.0,\n",
        "    max_tokens_per_sentence=50,\n",
        "    device='cuda'\n",
        "):\n",
        "    \"\"\"\n",
        "    (문장 단위) 한 번의 샘플링:\n",
        "      1) 문장이 끝날 때까지 토큰을 생성 ('.','?','!' 또는 eos_token 등장 시)\n",
        "      2) 해당 문장의 전체 G-NLL(= 토큰별 -log p의 합) 계산\n",
        "      3) 문장에 사용된 토큰 수 반환\n",
        "      4) 업데이트된 new_ids(= context_ids + 새 토큰들)\n",
        "\n",
        "    Returns:\n",
        "      generated_sentence (str)    : 생성된 문장(단락)\n",
        "      total_gnll (float)         : 문장 전체의 G-NLL\n",
        "      generated_token_count (int) : 문장에 사용된 토큰 수\n",
        "      new_ids (tensor)           : 새 context (이전 context + 방금 생성된 문장)\n",
        "    \"\"\"\n",
        "    new_ids = context_ids.clone()\n",
        "    total_gnll = 0.0\n",
        "    generated_token_count = 0\n",
        "\n",
        "    for step in range(max_tokens_per_sentence):\n",
        "        outputs = model(input_ids=new_ids)\n",
        "        logits = outputs.logits\n",
        "        last_token_logits = logits[:, -1, :]\n",
        "\n",
        "        scaled_logits = last_token_logits / temperature\n",
        "\n",
        "        sorted_logits, sorted_indices = torch.sort(scaled_logits, descending=True)\n",
        "        cprobs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "        idx_remove = (cprobs > top_p)\n",
        "        if idx_remove.any():\n",
        "            first_true_idx = torch.nonzero(idx_remove, as_tuple=True)[1][0].item()\n",
        "            sorted_logits[0, first_true_idx+1:] = float('-inf')\n",
        "\n",
        "        re_sorted_logits = torch.full_like(scaled_logits, float('-inf'))\n",
        "        re_sorted_logits[0, sorted_indices] = sorted_logits[0]\n",
        "\n",
        "        probs = torch.softmax(re_sorted_logits, dim=-1)\n",
        "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        next_token_prob = probs[0, next_token_id]\n",
        "        total_gnll += -torch.log(next_token_prob).item()\n",
        "        generated_token_count += 1\n",
        "\n",
        "        new_ids = torch.cat([new_ids, next_token_id], dim=1)\n",
        "\n",
        "        if tokenizer.eos_token_id is not None:\n",
        "            if next_token_id.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        decoded_so_far = tokenizer.decode(new_ids[0], skip_special_tokens=True)\n",
        "        if check_sentence_end(decoded_so_far):\n",
        "            break\n",
        "\n",
        "    added_tokens = new_ids[0, context_ids.shape[1]:]\n",
        "    generated_sentence = tokenizer.decode(added_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return generated_sentence, total_gnll, generated_token_count, new_ids\n",
        "\n",
        "def sample_sentence_candidates_and_pick_best(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    context_ids,\n",
        "    n_candidates=3,\n",
        "    uncertainty_method=\"total\",\n",
        "    top_p=0.9,\n",
        "    temperature=1.0,\n",
        "    max_tokens_per_sentence=50,\n",
        "    device='cuda'\n",
        "):\n",
        "    \"\"\"\n",
        "    문장 하나를 만들기 위해 N번 샘플링 후,\n",
        "    (1) total G-NLL (비정규화)  vs\n",
        "    (2) normalized G-NLL (토큰 수로 나눈 값)\n",
        "    중 하나를 선택적으로 비교해 가장 확신 낮은(= 측정값이 가장 높은) 후보를 골라낸다.\n",
        "\n",
        "    Args:\n",
        "        model, tokenizer\n",
        "        context_ids (tensor): 현재까지의 컨텍스트\n",
        "        n_candidates (int) : 문장 후보 샘플 수\n",
        "        uncertainty_method (str): \"total\" or \"normalized\"\n",
        "        top_p, temperature, max_tokens_per_sentence\n",
        "        device\n",
        "\n",
        "    Returns:\n",
        "        best_sentence (str) : 선택된 문장\n",
        "        best_gnll (float)   : 그 문장의 G-NLL(총합)\n",
        "        best_ids (tensor)   : context_ids + best_sentence\n",
        "    \"\"\"\n",
        "    candidates = []\n",
        "\n",
        "    for i in range(n_candidates):\n",
        "        gen_sentence, gnll, token_count, new_ids = sample_single_sentence(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            context_ids=context_ids,\n",
        "            top_p=top_p,\n",
        "            temperature=temperature,\n",
        "            max_tokens_per_sentence=max_tokens_per_sentence,\n",
        "            device=device\n",
        "        )\n",
        "        if uncertainty_method == \"total\":\n",
        "            measure = gnll\n",
        "        else:\n",
        "            measure = gnll / (token_count if token_count > 0 else 1)\n",
        "\n",
        "        candidates.append((gen_sentence, gnll, token_count, measure, new_ids))\n",
        "\n",
        "    best_idx = max(range(len(candidates)), key=lambda i: candidates[i][3])\n",
        "    best_sentence, best_gnll, best_count, best_measure, best_ids = candidates[best_idx]\n",
        "\n",
        "    return best_sentence, best_gnll, best_ids\n",
        "\n",
        "def generate_text_sentence_by_sentence(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    max_sentences=5,\n",
        "    n_candidates=3,\n",
        "    uncertainty_method=\"total\",\n",
        "    top_p=0.9,\n",
        "    temperature=1.0,\n",
        "    max_tokens_per_sentence=50,\n",
        "    device='cuda'\n",
        "):\n",
        "    \"\"\"\n",
        "    문장 단위로 생성:\n",
        "      - Prompt -> 문장1 생성(N샘플 중 best) -> 문장2 생성(N샘플 중 best) -> ...\n",
        "      - 최대 max_sentences까지 반복\n",
        "      - 문장 생성은 샘플링 방식(Top-p, Temperature)\n",
        "      - 선택 시 G-NLL vs G-NLL/TokenLength 둘 중 하나를 골라서 '최댓값' 고르는 방식\n",
        "\n",
        "    Returns:\n",
        "      최종적으로 생성된 텍스트 (str)\n",
        "    \"\"\"\n",
        "    context_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    final_text = tokenizer.decode(context_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    sentence_count = 0\n",
        "\n",
        "    while sentence_count < max_sentences:\n",
        "        sentence_count += 1\n",
        "\n",
        "        best_sentence, best_gnll, best_ids = sample_sentence_candidates_and_pick_best(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            context_ids=context_ids,\n",
        "            n_candidates=n_candidates,\n",
        "            uncertainty_method=uncertainty_method,\n",
        "            top_p=top_p,\n",
        "            temperature=temperature,\n",
        "            max_tokens_per_sentence=max_tokens_per_sentence,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        context_ids = best_ids\n",
        "\n",
        "        final_text += best_sentence\n",
        "\n",
        "        if tokenizer.eos_token_id is not None:\n",
        "            if context_ids[0, -1].item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    return final_text\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    tokenizer, model = load_llama7b_model(model_path, device=device)\n",
        "\n",
        "    prompt = \"\"\"How many r in the word 'strawberry?'\"\"\"\n",
        "\n",
        "    print(\"\\n=== [A] total G-NLL 기준 ===\")\n",
        "    text_total = generate_text_sentence_by_sentence(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        prompt=prompt,\n",
        "        max_sentences=10,\n",
        "        n_candidates=10,\n",
        "        uncertainty_method=\"total\",  # G-NLL\n",
        "        top_p=0.9,\n",
        "        temperature=1.0,\n",
        "        max_tokens_per_sentence=100,\n",
        "        device=device\n",
        "    )\n",
        "    print(\"[Result - total G-NLL]\\n\", text_total)\n",
        "\n",
        "    print(\"\\n=== [B] normalized G-NLL 기준 ===\")\n",
        "    text_normalized = generate_text_sentence_by_sentence(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        prompt=prompt,\n",
        "        max_sentences=10,\n",
        "        n_candidates=10,\n",
        "        uncertainty_method=\"normalized\",  # G-NLL / token_length\n",
        "        top_p=0.9,\n",
        "        temperature=1.0,\n",
        "        max_tokens_per_sentence=100,\n",
        "        device=device\n",
        "    )\n",
        "    print(\"[Result - normalized G-NLL]\\n\", text_normalized)\n"
      ],
      "metadata": {
        "id": "I-aLGhVT3f4Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}