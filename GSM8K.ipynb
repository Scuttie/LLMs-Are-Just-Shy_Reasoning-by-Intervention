{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Login Llama"
      ],
      "metadata": {
        "id": "Ewg8DzYZg_Qz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gJghYwXg3_B"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSM8K"
      ],
      "metadata": {
        "id": "JawnPcbhhCZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "D3_CyxQyhPCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"openai/gsm8k\", \"main\")"
      ],
      "metadata": {
        "id": "W3qHIg5phSZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ],
      "metadata": {
        "id": "iVQHY8_ah9EP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import time\n",
        "\n",
        "def load_llama7b_model(model_path, device='cuda'):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    model.eval()\n",
        "    return tokenizer, model\n",
        "\n",
        "def check_sentence_end(decoded_text_so_far: str):\n",
        "    \"\"\"\n",
        "    문장 종료 판별:\n",
        "      - 마침표('.', '?', '!') 여부로 판별하되, 소수점(2.5, 3.21 등)은 문장 끝으로 취급하지 않도록 처리\n",
        "      - '...', '?!', 한글 마침표 등 더 복잡한 처리가 필요할 수 있음\n",
        "    \"\"\"\n",
        "    decoded_text_so_far = decoded_text_so_far.strip()\n",
        "    if len(decoded_text_so_far) == 0:\n",
        "        return False\n",
        "\n",
        "    last_char = decoded_text_so_far[-1]\n",
        "\n",
        "    # 1) 소수점 예외 처리: 마지막이 '.'이면서 바로 앞이 숫자라면 소수점으로 취급 -> 문장 끝 아님\n",
        "    if last_char == '.':\n",
        "        if len(decoded_text_so_far) >= 2 and decoded_text_so_far[-2].isdigit():\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "    elif last_char in ['?', '!']:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def sample_single_sentence(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    context_ids,\n",
        "    top_p=0.9,\n",
        "    temperature=1.0,\n",
        "    max_tokens_per_sentence=50,\n",
        "    device='cuda'\n",
        "):\n",
        "    \"\"\"\n",
        "    (문장 단위) 한 번의 샘플링 (Top-p + Temperature):\n",
        "      1) 문장이 끝날 때까지 토큰을 생성 ('.','?','!' 또는 eos_token 등장 시)\n",
        "      2) 해당 문장의 전체 G-NLL(= 토큰별 -log p의 합) 계산\n",
        "      3) 문장에 사용된 토큰 수 반환\n",
        "      4) 업데이트된 new_ids(= context_ids + 새 토큰들)\n",
        "    \"\"\"\n",
        "    new_ids = context_ids.clone()\n",
        "    total_gnll = 0.0\n",
        "    generated_token_count = 0\n",
        "\n",
        "    for step in range(max_tokens_per_sentence):\n",
        "        outputs = model(input_ids=new_ids)\n",
        "        logits = outputs.logits\n",
        "        last_token_logits = logits[:, -1, :]\n",
        "\n",
        "        # temperature 적용\n",
        "        scaled_logits = last_token_logits / temperature\n",
        "\n",
        "        # top-p filtering\n",
        "        sorted_logits, sorted_indices = torch.sort(scaled_logits, descending=True)\n",
        "        cprobs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "        idx_remove = (cprobs > top_p)\n",
        "        if idx_remove.any():\n",
        "            first_true_idx = torch.nonzero(idx_remove, as_tuple=True)[1][0].item()\n",
        "            sorted_logits[0, first_true_idx+1:] = float('-inf')\n",
        "\n",
        "        # 원래 순서대로 재배치\n",
        "        re_sorted_logits = torch.full_like(scaled_logits, float('-inf'))\n",
        "        re_sorted_logits[0, sorted_indices] = sorted_logits[0]\n",
        "\n",
        "        # 샘플링\n",
        "        probs = torch.softmax(re_sorted_logits, dim=-1)\n",
        "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        # G-NLL 누적 (마지막 뽑힌 토큰의 -log prob)\n",
        "        next_token_prob = probs[0, next_token_id]\n",
        "        total_gnll += -torch.log(next_token_prob).item()\n",
        "        generated_token_count += 1\n",
        "\n",
        "        # 시퀀스에 추가\n",
        "        new_ids = torch.cat([new_ids, next_token_id], dim=1)\n",
        "\n",
        "        # eos 검사\n",
        "        if tokenizer.eos_token_id is not None:\n",
        "            if next_token_id.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        # 문장 종료 검사\n",
        "        decoded_so_far = tokenizer.decode(new_ids[0], skip_special_tokens=True)\n",
        "        if check_sentence_end(decoded_so_far):\n",
        "            break\n",
        "\n",
        "    # 새로 생성된 부분 디코딩\n",
        "    added_tokens = new_ids[0, context_ids.shape[1]:]\n",
        "    generated_sentence = tokenizer.decode(added_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return generated_sentence, total_gnll, generated_token_count, new_ids\n",
        "\n",
        "def sample_single_sentence_greedy(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    context_ids,\n",
        "    max_tokens_per_sentence=50,\n",
        "    device='cuda'\n",
        "):\n",
        "    \"\"\"\n",
        "    (문장 단위) 한 번의 그리디 디코딩:\n",
        "      - 각 단계마다 확률이 가장 높은 토큰 하나를 선택 (argmax)\n",
        "      - 문장 끝(?, !, .) 또는 eos_token이면 중단\n",
        "    \"\"\"\n",
        "    new_ids = context_ids.clone()\n",
        "    total_gnll = 0.0\n",
        "    generated_token_count = 0\n",
        "\n",
        "    for step in range(max_tokens_per_sentence):\n",
        "        outputs = model(input_ids=new_ids)\n",
        "        logits = outputs.logits\n",
        "        last_token_logits = logits[:, -1, :]\n",
        "\n",
        "        # argmax 선택 (greedy)\n",
        "        next_token_id = torch.argmax(last_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "        # G-NLL 계산\n",
        "        probs = torch.softmax(last_token_logits, dim=-1)\n",
        "        next_token_prob = probs[0, next_token_id]\n",
        "        total_gnll += -torch.log(next_token_prob).item()\n",
        "        generated_token_count += 1\n",
        "\n",
        "        # 시퀀스에 추가\n",
        "        new_ids = torch.cat([new_ids, next_token_id], dim=1)\n",
        "\n",
        "        # eos 검사\n",
        "        if tokenizer.eos_token_id is not None:\n",
        "            if next_token_id.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        # 문장 종료 검사\n",
        "        decoded_so_far = tokenizer.decode(new_ids[0], skip_special_tokens=True)\n",
        "        if check_sentence_end(decoded_so_far):\n",
        "            break\n",
        "\n",
        "    added_tokens = new_ids[0, context_ids.shape[1]:]\n",
        "    generated_sentence = tokenizer.decode(added_tokens, skip_special_tokens=True)\n",
        "    return generated_sentence, total_gnll, generated_token_count, new_ids\n",
        "\n",
        "def sample_sentence_candidates_and_pick_best(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    context_ids,\n",
        "    n_candidates=3,\n",
        "    uncertainty_method=\"total\",\n",
        "    top_p=0.9,\n",
        "    temperature=1.0,\n",
        "    max_tokens_per_sentence=50,\n",
        "    device='cuda',\n",
        "    min_or_max=\"max\"\n",
        "):\n",
        "    \"\"\"\n",
        "    문장 하나를 만들기 위해 N번 샘플링 후,\n",
        "    (1) total G-NLL (비정규화)\n",
        "    (2) normalized G-NLL (토큰 수로 나눈 값)\n",
        "    중 하나로 후보를 비교 -> min_or_max에 따라 결정\n",
        "    \"\"\"\n",
        "    candidates = []\n",
        "\n",
        "    for i in range(n_candidates):\n",
        "        gen_sentence, gnll, token_count, new_ids = sample_single_sentence(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            context_ids=context_ids,\n",
        "            top_p=top_p,\n",
        "            temperature=temperature,\n",
        "            max_tokens_per_sentence=max_tokens_per_sentence,\n",
        "            device=device\n",
        "        )\n",
        "        if uncertainty_method == \"total\":\n",
        "            measure = gnll\n",
        "        else:  # \"normalized\"\n",
        "            measure = gnll / (token_count if token_count > 0 else 1)\n",
        "\n",
        "        candidates.append((gen_sentence, gnll, token_count, measure, new_ids))\n",
        "\n",
        "    if min_or_max == \"max\":\n",
        "        best_idx = max(range(len(candidates)), key=lambda i: candidates[i][3])\n",
        "    else:  # \"min\"\n",
        "        best_idx = min(range(len(candidates)), key=lambda i: candidates[i][3])\n",
        "\n",
        "    best_sentence, best_gnll, best_count, best_measure, best_ids = candidates[best_idx]\n",
        "    return best_sentence, best_gnll, best_ids\n",
        "\n",
        "def generate_text_sentence_by_sentence(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    max_sentences=5,\n",
        "    n_candidates=3,\n",
        "    uncertainty_method=\"total\",\n",
        "    top_p=0.9,\n",
        "    temperature=1.0,\n",
        "    max_tokens_per_sentence=50,\n",
        "    device='cuda',\n",
        "    min_or_max=\"max\"\n",
        "):\n",
        "    \"\"\"\n",
        "    문장 단위 샘플링 기반 생성 (Top-p, Temperature 기반):\n",
        "      - 각 문장 생성 시, N개의 후보 중 best pick\n",
        "      - best pick 선정 시 'total' or 'normalized' + min_or_max 로직\n",
        "      - 패턴(\"the answer is:\", \"The answer is:\", \"The final answer is:\") 등장 시 중단\n",
        "    \"\"\"\n",
        "    context_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    final_text = tokenizer.decode(context_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    sentence_count = 0\n",
        "\n",
        "    while sentence_count < max_sentences:\n",
        "        sentence_count += 1\n",
        "\n",
        "        best_sentence, best_gnll, best_ids = sample_sentence_candidates_and_pick_best(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            context_ids=context_ids,\n",
        "            n_candidates=n_candidates,\n",
        "            uncertainty_method=uncertainty_method,\n",
        "            top_p=top_p,\n",
        "            temperature=temperature,\n",
        "            max_tokens_per_sentence=max_tokens_per_sentence,\n",
        "            device=device,\n",
        "            min_or_max=min_or_max\n",
        "        )\n",
        "        context_ids = best_ids\n",
        "        final_text += best_sentence\n",
        "\n",
        "        # ------------------------------\n",
        "        # 패턴 조건: 중단 로직\n",
        "        # ------------------------------\n",
        "        if final_text.count(\"the answer is:\") == 1:\n",
        "            break\n",
        "        if final_text.count(\"The answer is:\") == 1:\n",
        "            break\n",
        "        if final_text.count(\"The final answer is:\") == 2:\n",
        "            break\n",
        "\n",
        "        # eos 검사\n",
        "        if tokenizer.eos_token_id is not None:\n",
        "            if context_ids[0, -1].item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    return final_text\n",
        "\n",
        "def generate_text_sentence_by_sentence_greedy(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    max_sentences=5,\n",
        "    max_tokens_per_sentence=50,\n",
        "    device='cuda'\n",
        "):\n",
        "    \"\"\"\n",
        "    문장 단위 '그리디' 기반 생성:\n",
        "      - 각 문장마다 greedy decoding으로 진행\n",
        "      - 패턴(\"the answer is:\", \"The answer is:\", \"The final answer is:\") 등장 시 중단\n",
        "    \"\"\"\n",
        "    context_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    final_text = tokenizer.decode(context_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    sentence_count = 0\n",
        "\n",
        "    while sentence_count < max_sentences:\n",
        "        sentence_count += 1\n",
        "\n",
        "        gen_sentence, gnll, token_count, new_ids = sample_single_sentence_greedy(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            context_ids=context_ids,\n",
        "            max_tokens_per_sentence=max_tokens_per_sentence,\n",
        "            device=device\n",
        "        )\n",
        "        context_ids = new_ids\n",
        "        final_text += gen_sentence\n",
        "\n",
        "        # ------------------------------\n",
        "        # 패턴 조건: 중단 로직\n",
        "        # ------------------------------\n",
        "        if final_text.count(\"the answer is:\") == 1:\n",
        "            break\n",
        "        if final_text.count(\"The answer is:\") == 1:\n",
        "            break\n",
        "        if final_text.count(\"The final answer is:\") == 2:\n",
        "            break\n",
        "\n",
        "        # eos 검사\n",
        "        if tokenizer.eos_token_id is not None:\n",
        "            if context_ids[0, -1].item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    return final_text"
      ],
      "metadata": {
        "id": "SRXc2zrthZT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # -------------------------------------------------\n",
        "    # 1) 모델 로딩\n",
        "    # -------------------------------------------------\n",
        "    model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    tokenizer, model = load_llama7b_model(model_path, device=device)\n",
        "\n",
        "    # 결과 저장 경로\n",
        "    output_dir = \"Your Path\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_file = os.path.join(output_dir, \"inference_results.txt\")\n",
        "\n",
        "    # 시스템 지시\n",
        "    system_instruction = (\n",
        "        \"You are a helpful assistant. When you generate an answer, \"\n",
        "        \"please start it with the following phrase: 'The final answer is: ' \"\n",
        "        \"and then provide your solution.\\n\"\n",
        "        \"If you have finished your reasoning, do not continue beyond that phrase.\"\n",
        "        \"Let's think step by step. Reason carefully then answer.\"\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # 3) 테스트 데이터셋에 대해 순회하며 인퍼런스\n",
        "    # -------------------------------------------------\n",
        "    for i, question_text in enumerate(dataset['test']['question']):\n",
        "        print(f\"\\n============================\")\n",
        "        print(f\"Test sample #{i+1}\")\n",
        "        print(f\"Question: {question_text}\")\n",
        "\n",
        "        log_str_list = []\n",
        "        log_str_list.append(\"\\n============================\")\n",
        "        log_str_list.append(f\"Test sample #{i+1}\")\n",
        "        log_str_list.append(f\"Question: {question_text}\")\n",
        "\n",
        "        # 프롬프트 준비\n",
        "        prompt = system_instruction + \"\\n\\n\" + question_text\n",
        "\n",
        "        # ---- (1) total G-NLL + max ----\n",
        "        start_t = time.time()\n",
        "        ans_total_max = generate_text_sentence_by_sentence(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            prompt=prompt,\n",
        "            max_sentences=10,\n",
        "            n_candidates=10,\n",
        "            uncertainty_method=\"total\",  # total G-NLL\n",
        "            top_p=0.9,\n",
        "            temperature=1.0,\n",
        "            max_tokens_per_sentence=100,\n",
        "            device=device,\n",
        "            min_or_max=\"max\"             # max\n",
        "        )\n",
        "        end_t = time.time()\n",
        "        elapsed_total_max = end_t - start_t\n",
        "\n",
        "        print(\"\\n[1] total G-NLL + max:\")\n",
        "        print(ans_total_max)\n",
        "        print(f\"Elapsed time: {elapsed_total_max:.2f} seconds\")\n",
        "\n",
        "        log_str_list.append(\"\\n[1] total G-NLL + max:\")\n",
        "        log_str_list.append(ans_total_max)\n",
        "        log_str_list.append(f\"Elapsed time: {elapsed_total_max:.2f} seconds\")\n",
        "\n",
        "        # ---- (2) normalized G-NLL + max ----\n",
        "        start_t = time.time()\n",
        "        ans_norm_max = generate_text_sentence_by_sentence(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            prompt=prompt,\n",
        "            max_sentences=10,\n",
        "            n_candidates=10,\n",
        "            uncertainty_method=\"normalized\",\n",
        "            top_p=0.9,\n",
        "            temperature=1.0,\n",
        "            max_tokens_per_sentence=100,\n",
        "            device=device,\n",
        "            min_or_max=\"max\"\n",
        "        )\n",
        "        end_t = time.time()\n",
        "        elapsed_norm_max = end_t - start_t\n",
        "\n",
        "        print(\"\\n[2] normalized G-NLL + max:\")\n",
        "        print(ans_norm_max)\n",
        "        print(f\"Elapsed time: {elapsed_norm_max:.2f} seconds\")\n",
        "\n",
        "        log_str_list.append(\"\\n[2] normalized G-NLL + max:\")\n",
        "        log_str_list.append(ans_norm_max)\n",
        "        log_str_list.append(f\"Elapsed time: {elapsed_norm_max:.2f} seconds\")\n",
        "\n",
        "        # ---- (3) total G-NLL + min ----\n",
        "        start_t = time.time()\n",
        "        ans_total_min = generate_text_sentence_by_sentence(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            prompt=prompt,\n",
        "            max_sentences=10,\n",
        "            n_candidates=10,\n",
        "            uncertainty_method=\"total\",\n",
        "            top_p=0.9,\n",
        "            temperature=1.0,\n",
        "            max_tokens_per_sentence=100,\n",
        "            device=device,\n",
        "            min_or_max=\"min\"             # min\n",
        "        )\n",
        "        end_t = time.time()\n",
        "        elapsed_total_min = end_t - start_t\n",
        "\n",
        "        print(\"\\n[3] total G-NLL + min:\")\n",
        "        print(ans_total_min)\n",
        "        print(f\"Elapsed time: {elapsed_total_min:.2f} seconds\")\n",
        "\n",
        "        log_str_list.append(\"\\n[3] total G-NLL + min:\")\n",
        "        log_str_list.append(ans_total_min)\n",
        "        log_str_list.append(f\"Elapsed time: {elapsed_total_min:.2f} seconds\")\n",
        "\n",
        "        # ---- (4) normalized G-NLL + min ----\n",
        "        start_t = time.time()\n",
        "        ans_norm_min = generate_text_sentence_by_sentence(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            prompt=prompt,\n",
        "            max_sentences=10,\n",
        "            n_candidates=10,\n",
        "            uncertainty_method=\"normalized\",\n",
        "            top_p=0.9,\n",
        "            temperature=1.0,\n",
        "            max_tokens_per_sentence=100,\n",
        "            device=device,\n",
        "            min_or_max=\"min\"\n",
        "        )\n",
        "        end_t = time.time()\n",
        "        elapsed_norm_min = end_t - start_t\n",
        "\n",
        "        print(\"\\n[4] normalized G-NLL + min:\")\n",
        "        print(ans_norm_min)\n",
        "        print(f\"Elapsed time: {elapsed_norm_min:.2f} seconds\")\n",
        "\n",
        "        log_str_list.append(\"\\n[4] normalized G-NLL + min:\")\n",
        "        log_str_list.append(ans_norm_min)\n",
        "        log_str_list.append(f\"Elapsed time: {elapsed_norm_min:.2f} seconds\")\n",
        "\n",
        "        # ---- (5) Greedy Decoding ----\n",
        "        start_t = time.time()\n",
        "        ans_greedy = generate_text_sentence_by_sentence_greedy(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            prompt=prompt,\n",
        "            max_sentences=10,\n",
        "            max_tokens_per_sentence=100,\n",
        "            device=device\n",
        "        )\n",
        "        end_t = time.time()\n",
        "        elapsed_greedy = end_t - start_t\n",
        "\n",
        "        print(\"\\n[5] Greedy Decoding:\")\n",
        "        print(ans_greedy)\n",
        "        print(f\"Elapsed time: {elapsed_greedy:.2f} seconds\")\n",
        "\n",
        "        log_str_list.append(\"\\n[5] Greedy Decoding:\")\n",
        "        log_str_list.append(ans_greedy)\n",
        "        log_str_list.append(f\"Elapsed time: {elapsed_greedy:.2f} seconds\")\n",
        "\n",
        "        # 최종 구분선\n",
        "        print(\"============================\\n\")\n",
        "        log_str_list.append(\"\\n============================\\n\")\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # 4) 각 결과를 파일로 기록\n",
        "        # ---------------------------------------------------------\n",
        "        with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
        "            for line in log_str_list:\n",
        "                f.write(line + \"\\n\")\n"
      ],
      "metadata": {
        "id": "ZcwVx7hZhwbA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}